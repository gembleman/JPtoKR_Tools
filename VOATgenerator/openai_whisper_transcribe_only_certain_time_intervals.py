# -*- coding: utf-8 -*-
"""OpenAI Whisper - transcribe only certain time intervals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17cTsmfVJmpDDMURGcu8hUu1zHNAYbfa5

# OpenAI Whisper with time intervals

This allows you to make transcriptions on certain time intervals or even exclude certain time intervals from the transcriptions by getting the audio as an array and filtering stuff out before passing it to whisper.

Written by [Octavian Mot](https://github.com/octimot/)

## Install packages

If you don't have access to a GPU runtime, just comment out the `pip install torch ...` below and it should work on a CPU
"""

"""## Import dependencies"""

import torch
import whisper
import librosa

# load whisper model
torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = whisper.load_model("medium")

"""
## Upload test audio file

This will be deleted once the runtime is deleted."""

# upload the file

uploaded = "en_example.wav"

# get the path (name) of the first uploaded file
audio_file_path = "en_example.wav"

print('Using audio file:', audio_file_path)

"""# Define main functions"""

def split_audio_by_intervals(audio_array, time_intervals=None, sr=16_000):
    '''
    Splits the audio_array according to the time_intervals
    and returns a audio_segments list with multiple audio_arrays
    together with the time_intervals passed to the function

    '''

    # sort the audio segments by start time
    time_intervals = sorted(time_intervals, key=lambda x:x[0])

    # reset the audio segments list
    audio_segments = []

    # if there are time segments
    if time_intervals is not None and time_intervals and len(time_intervals) > 0:

      # take each time segment
      for time_interval in time_intervals:

          # calculate duration based on start and end times!!

          # and add it to an audio segments list
          # the format is [start_time, end_time, audio_array]
          audio_segment = [time_interval[0], 
                           time_interval[1],
                           audio_array[int(time_interval[0] * sr): 
                                       int(time_interval[1] * sr)]
                           ]

          audio_segments.append(audio_segment)


    # if time_intervals is empty, define it as a single segment, 
    # from the beginning to the end (i.e. we're transcribing the full audio)
    else:
      time_intervals = [[0, len(audio_array)/sr]]
      audio_segments = [0, len(audio_array/sr), audio_array]

    return audio_segments, time_intervals


def transcribe_segments(audio_segments, other_whisper_options):
    '''
    Transcribes only the passed audio segments 
    and offsets the transcription segments start and end times

    '''

    # transcribe each audio segment
    for i, audio_segment in enumerate(audio_segments):

      print(f"Segment {i} - second {audio_segment[0]} to {audio_segment[1]}")

      # run whisper transcribe on the audio segment
      result = model.transcribe(audio_segment[2],task=task, verbose=False,)
      

      print("\nResults with offsets:")
      # now process the result and add the original start time offset 
      # to each transcript segment start and end times

      # if there are segments in the result
      if 'segments' in result and result['segments']:

        # take each segment and add the offset to the start and end time
        for transcript_segment in result['segments']:
            transcript_segment['start'] += audio_segment[0]
            transcript_segment['end'] += audio_segment[0]

            # avoid end time being larger than the interval end time
            # - there seems to be an issue in the whisper model:
            #   https://github.com/openai/whisper/discussions/357
            if transcript_segment['end'] > audio_segment[1]:
                transcript_segment['end'] = audio_segment[1]

            # also avoid start time being smaller than the interval start time
            if transcript_segment['start'] < audio_segment[0]:
              transcript_segment['start'] = audio_segment[0] 

            print(transcript_segment['start'], ' --> ', 
                  transcript_segment['end'], '\n', transcript_segment['text'])
            
      print("\n")


def exclude_segments_by_intervals(audio_array, 
                                  time_intervals, 
                                  excluded_time_intervals, 
                                  sr):
    '''
    Excludes certain audio segments according to the excluded_time_intervals
    '''

    # sort the excluded segments by start time
    excluded_time_intervals = \
        sorted(excluded_time_intervals, key=lambda x:x[0])

    print('Excluding intervals:\n ', excluded_time_intervals)


    # if there are exclusion time segments
    if excluded_time_intervals and len(excluded_time_intervals) > 0:

      # take each time segment
      for excluded_time_interval in excluded_time_intervals:

        #print('\n---\nProcessing exclusion: ', excluded_time_interval)

        # and check it against each of the time segments we selected for transcription
        for time_interval in time_intervals:

          #print('\n', time_interval)

          # if the exclusion is outside the current segment times
          if excluded_time_interval[1] <= time_interval[0] \
            or excluded_time_interval[0] >= time_interval[1]:

            #print('outside, ignoring')
            continue

          # if the exclusion is exactly as the current segment times
          elif time_interval[0] == excluded_time_interval[0] \
            and time_interval[1] == excluded_time_interval[1]:

            # simply remove the whole segment
            time_intervals.remove(time_interval)

            #print('exact match, removing')
          
          else:

            # if the exclusion start time is equal to the segment start time
            if excluded_time_interval[0] == time_interval[0]:

              # cut out the beginning of the segment
              # by using the end time of the exclusion as its start
              time_interval[0] = excluded_time_interval[1]

              #print('cutting beginning, modified segment:', time_interval)


            # if the exclusion end time is equal to the segment end time
            elif excluded_time_interval[1] == time_interval[1]:

              # cut out the end of the segment
              # by using the start time of the exclusion as its end
              time_interval[1] = excluded_time_interval[0]

              #print('cutting end, modified segment:', time_interval)


            # if the exclusion is in the middle of the segment
            elif excluded_time_interval[0] > time_interval[0]\
              and excluded_time_interval[1] < time_interval[1]:

              #print('splitting in two')

              # remove the segment from the list
              time_intervals.remove(time_interval)

              # but then split it into two segments
              # first the segment until the exclusion
              time_intervals.append([time_interval[0], 
                                    excluded_time_interval[0]])

              # then the segment from the exclusion
              time_intervals.append([excluded_time_interval[1], 
                                    time_interval[1]])


              # print('splitting in two: ', 
              #  [time_interval[0], excluded_time_interval[0]], 
              #  [excluded_time_interval[1], time_interval[1]])



    # sort the selection by start time
    time_intervals = sorted(time_intervals, key=lambda x:x[0])

    # print('New selection for transcription: \n ', time_intervals)

    # now split the audio by the newly created intervals
    audio_segments, time_intervals = split_audio_by_intervals(audio_array, 
                                                          time_intervals, 
                                                          sr)

    return audio_segments, time_intervals

"""# Load audio using librosa

We're using librosa to load the audio as an array. 
This is not necessary for the default Whisper pipeline, but we'll need it later to split the audio into the segments we want.
"""

# define whisper options:
other_whisper_options = {'language': 'english'}
task = 'transcribe'

# load audio file using librosa and get the audio_array
audio_array, sr = librosa.load(audio_file_path, sr=16_000)

"""# Transcribe full audio with Whisper (transcription 1)

Just to see all the transcriptions segments as you'd expect from a normal Whisper transcription, and then use them for visual comparison.
"""

# there's just one audio segment, which is the full audio array 
#audio_segments = audio_array

# transcribe the audio
#results = model.transcribe(audio_segments, task=task, verbose=True, **other_whisper_options)

"""# Only transcribe certain segments (transcription 2)
#### But, first define which using their start and end times
"""

# which time intervals do we want to transcribe?
# anything in the audio not within these intervals will be ignored
# format is 
# [
#    [segment1_start_time, segment1_end_time], 
#    [segment2_start_time, segment2_end_time], 
#    etc.
# ]
# these times can be placed in an unordered fashion, 
# as they will be sorted later
time_intervals = [[54, 60], [19, 27], [40.600, 53.120]]

print('Selected intervals for transcription:\n {} \n'.format(time_intervals))

# call split function
audio_segments, time_intervals = split_audio_by_intervals(audio_array, time_intervals, sr)

print('time intervals:\n {} \n'.format(time_intervals))
print('audio segments:\n {} \n'.format(audio_segments))

transcribe_segments(audio_segments, other_whisper_options)

'''
# Do not transcribe certain segments (transcription 3)

#### First, define which time intervals you absolutely don't want to transcribe

Note: you do not need to do any of the previous transcriptions for this to work.


# you can either use the full audio file ...
time_intervals = [[0, len(audio_array)/sr]]

# ... or time intervals as above
# time_intervals =  [[1, 6], [19, 27], [30, 32], [40.6, 53.12], [54, 60]]

print('Selected intervals for transcription:\n ',time_intervals)

# which time segments we do NOT want to transcribe?
# format is same as above:
#  [[segment1_start_time, segment1_end_time], [segment2_start_time, segment2_end_time], etc.]
# these times can be in an unordered fashion, as they will be sorted later

excluded_time_intervals = [[4, 6], [19, 21], [30, 32], [43, 50]]

# call the exclude function to filter out the unwanted audio segments
audio_segments, time_intervals = \
      exclude_segments_by_intervals(audio_array, 
                                time_intervals, 
                                excluded_time_intervals, 
                                sr)



print('time intervals:\n {} \n'.format(time_intervals))
print('audio segments:\n ', audio_segments)

#### Now transcribe only the segments that haven't been excluded 
Note: you do not need to do the first two transcriptions for this to work 


# transcribe each audio segment without the exclusions
transcribe_segments(audio_segments, other_whisper_options)
'''